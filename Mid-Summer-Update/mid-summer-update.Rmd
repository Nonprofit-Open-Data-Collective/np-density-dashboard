---
title: "Fiscal Health Package and Geospatial Dashboard: Updates"
author: "Christian Maino Vieytes"
output: beamer_presentation
---

```{r echo = FALSE, message = FALSE, warning = FALSE}
library( tidyverse )
library( urbnmapr ) # county/state shapefiles
library( sf )
library( fipio ) # coordinates to FIPS
library( tigris ) # shapefiles
library( tidycensus ) # census data
options( tigris_class = "sf" )
options( tigris_use_cache = TRUE )
knitr::knit_hooks$set( mysize = function( before, options, envir ) {
  if ( before ) 
    return( options$size )
} )
```

## Fiscal Health Package

- **Goal**: provide users with automated calculation of critical fiscal health metrics for nonprofit organizations ( NPOs ) filing PC-990 and 990-EZ tax forms

- Particular emphasis on those using the NCCS 990 database

- These data contain a mixture of records from both Form 990 and 990-EZ filers


## What is Form 990?

* Form 990 is for tax-exempt organizations, nonexempt charitable trusts, and section 527 political organizations that file Form 990 to provide the IRS with the information required by section 6033.

* Yields key information about nonprofit operations in the US
  + Geographic
  + Size
  + Financial
  + Personnel 

* Extract geographic information from NPO key personnel 



## Fiscal Health Metrics


\includegraphics[width = 15cm,height = 6cm,keepaspectratio,]{/Users/Chris/Desktop/metrics-landing}

## Fiscal Health Metrics

* Variables in 990 data come from fields in Form 990 or 990 EZ

* 20 health metrics total

* All are simple ratios

* Metrics vary in the number of fields required for computation of numerator or denominator 
  + As many as 6 fields may be required


## Function Composition: The Skeleton

* **Input**: User supplies column names for variables required in metric

  + A consideration: variables with "PC", "EZ", or "PZ" scope 

* **Return**: a `data.frame` with the transformed ( e.g., normalized ) or winsorized ( i.e., truncated ) iterations of the metrics and accompanying plots of their distributions

## Function Composition: Nuance

* Main target audience: researchers harnessing the 990 database

* Argument defaults: write functions with default column names being those in the 

* Intended result: user supplies `data.frame` and function searches for pre-specified defaults and returns computed metrics or an error warning users of missing columns

## Function Composition: More Nuance

* What about metrics that can be computed from both PC- and EZ-scope variables?

* **Potential Solution**: write functions that can handle a vector of two character strings for each of the EZ and PC scope variables but can also handle a user supplying a single string for a single column

## Next Steps

* Take written conditionals and can them into helper function to allow program make decisions on what type of inputs are being supplied and what to output

* Finish building and then install package

* Publish


##

\centering Geospatial Dashboard Updates

## Overview of Steps

* Import/wrangle data
  + Update previously supplied code to incorporate new cycles of 1023-EZ data releases
  
* Generate 1023-EZ data shapefile and obtain other cartographic shapefiles to generate some maps ( static )

* Scale up to dashboard

## Data Import/Cleaning

* Time-intensive ( slow steps )

* Two datasets: one containing addresses for NPOs and one for board members

* 1023-EZ data is easy to access
  + Fields require cleaning and standardization ( in some cases across cycles )
  + Regex is useful

* Geocoding:
  + Census Geocoder: free; use `tidygeocoder` package ( first pass )
  + Google Geocoding API: first 40k queries are free ( clean-up )
  + PO Boxes and failed addresses using zip and city centroids ( more clean-up )
  
## Data Import/Cleaning ( cont. )

* Final Step: Integrated Public Use Microdata Series (IPUMS)
  + *Input*: supply a .csv of lat/long coordinates
  + *Return*: tract-level Census data associated with those coordinates
  + This will tell us something about the communities NPOs and board members reside in ( *limitation*: not necessarily     the communities they serve )


## Map-Making

* Use `sf` package

* US county/state shapefiles provided through `urbnmapr` package

* Transform 1023 EZ data into a shapefile ( points not polygons )

* Upon plotting initial map, we see lingering issues with the data...


## Non-Profits in Questionable Locations

```{r, echo = FALSE, message = FALSE, warning = FALSE, fig.align = 'center',fig.width = 3, fig.height = 2, tidy = TRUE, tidy.opts = list( width.cutoff = 60 ),size = 'tiny'}


lf <- "/Volumes/My Passport for Mac/Urban Institute/Summer Projects/Geospatial Dashboard/Large-Files-Bank"

setwd( lf )
np <- readRDS( "NONPROFITS-2014-2021v6.rds" )
np <- np %>%
  mutate( COUNTYFIPS = ifelse( str_count( COUNTYFIPS, '\\d' ) == 1, paste0( '00', COUNTYFIPS ),
                           ifelse( str_count( COUNTYFIPS, '\\d' ) == 2,paste0( '0', COUNTYFIPS ),
                                  ifelse( str_count( COUNTYFIPS, '\\d' ) == 3, paste0( COUNTYFIPS ), NA ) ) ),
         fips_ct = ifelse( is.na( STATEFIPS ) == F & is.na( COUNTYFIPS ) == F, paste0( STATEFIPS, COUNTYFIPS ),NA ) )


# obtain FIPS for those still missing FIPS code but having coordinates present                         
trim.sf <- np[ which( is.na( np$COUNTYFIPS ) & is.na( np$lat ) == F & is.na( np$lon ) == F ), c( 'lat','lon','key' ) ]

s <- list( )
 for ( i in 1:nrow( trim.sf ) ) {
  fips_c <- coords_to_fips( x = trim.sf$lon[i], y = trim.sf$lat[i] )
  s[[i]] <- data.frame( f = ifelse( length( fips_c ) == 0, NA, fips_c ),
                        key = trim.sf$key[i] )
  }   
s <- do.call( 'rbind', s )    

np.b <- left_join( np, s, by = 'key' ) %>%
  mutate( fips_ct = ifelse( is.na( fips_ct ), f, fips_ct ) ) %>%
  select( -f )

invisible( sum( is.na( np$fips_ct ) ) ) # previously had 642 missing county FIPS
invisible( sum( is.na( np$fips_ct ) ) ) # now reduced to 278

# Convert lat/long to a sf
np.sf <- np %>%
  filter( is.na( lat ) == F ) %>% # st_as_sf does not allow missing values in coordinate columns; n = 167 missing
  st_as_sf( coords = c( "lon", "lat" ), crs = "NAD83" )

# obtain state geometries
states.sf <- get_urbn_map( map = "counties", sf = TRUE )

invisible( st_crs( states.sf ) )
st_crs( states.sf ) <- st_crs( states.sf )
# Remove AK, HI from state and PR and GU from npos as well
np   <- np  [ !( np$State      %in% c( 'PR', 'GU' ) ), ] # exclude Puerto Rico and Guam
```

```{r, echo = FALSE,fig.align = 'center',fig.width = 3, fig.height = 2.5}
#plot locations over map
ggplot( ) +
  geom_sf( data = np.sf ) + 
  geom_sf( data = states.sf, fill = NA, color = "black", size = 0.15, alpha = 0 ) +
  coord_sf( datum = st_crs( 2163 ) ) +   
  labs( fill  = "", 
       title = "",
       caption = '' ) + 
  theme_bw( )

```

*Issues*:

- The same phenomena persists after excluding Hawaii, Alaska, Puerto Rico, and Guam

- Projection issues with Cartesian coordinate system for Hawaii and Alaska

- Loading time is very much an issue using this level of data


## Diagnosing The Issue

```{r, echo = T, warning = FALSE, message = FALSE, tidy = TRUE,tidy.opts = list( width.cutoff = 30 ),mysize = TRUE, size = '\\tiny'}
# Constrain lat long coordinates and diagnose the issue
np %>%
  filter( lat > 50 & State != "AK" ) %>%
  select( State, City, input_address )
```


## Let's Zoom In

```{r,  echo = FALSE, warning = FALSE, message = FALSE,tidy = TRUE,tidy.opts = list( width.cutoff = 20 ),mysize = TRUE, size = '\\tiny'}
## remove points manually beyond latitude/longitude confines of lower 48
np.sf.o <- np %>%
  filter( lat < 50 & lat > 24 & lon < ( -66 )  & lon > ( -125 ) ) %>%
  st_as_sf( coords = c( "lon", "lat" ), crs = "NAD83" )
  
```
  
```{r, echo = FALSE,fig.align = 'center',fig.width = 5, fig.height = 3}
ggplot( ) +
    geom_sf( data = np.sf.o ) + 
    geom_sf( data = states.sf, fill = NA, color = "black", size = 0.15, alpha = 0 ) +
    coord_sf( datum = st_crs( 2163 ) ) +   
    labs( fill  = "", 
         title = "",
         caption = '' ) + 
    theme_bw( )
```

```{r mysize = TRUE, size = '\\large', echo = FALSE}
```
- *Issue*: Map of points on US map is not very informative

- **Try**: Density or bubble map

- `sf` subsetting: Metropolitan Statistical Areas ( MSAs )


## Mapping MSAs

- Subsetting MSAs and spatial overlay is not so straightforward 

- `tigris` package: `core_based_statistical_areas` for fetching metro- or micropolitan region US Census TIGER shapefiles

- Tract boundaries within the statistical area are shown

- *Issue*: subsetting `sf` data returns census tracts grazing MSA borders

- *Solution*: `tigris` provides tools for addressing spatial overlay ( i.e., `st_within` )


## Example: Spatial Overlay


```{r, echo = FALSE, fig.align = 'center', fig.height = 3, fig.width = 3, warning = FALSE, message = FALSE }
wa <- tracts( "WA", cb = TRUE ) # Washington shapefiles

cb <- core_based_statistical_areas( cb = TRUE ) # fetches cartographic boundary files for MSA
sea <- filter( cb, grepl( "Seattle", NAME ) ) # match "Seattle" and return boundary files

p1 <- wa[ sea, ] # subset tracts from within ( or touching ) MSA boundary

ggplot( ) + 
  geom_sf( data = p1 ) + 
  geom_sf( data = sea, fill = NA, color = "red" ) + # red lines show cartographic boundaries of MSA
  theme_minimal( )
```


## Example: Remove Peripheral Census Tracts at the Cartographic Boundaries with `st_within`


```{r, echo = FALSE, fig.align = 'center', fig.height = 3, fig.width = 3,warning = FALSE, message = FALSE }
s <- st_within( wa, sea )

these <- map_lgl( s, function( x ) {
  if ( length( x ) == 1 ) {
    return( TRUE )
  } else {
    return( FALSE )
  }
} )

d <- wa[ these, ]
ggplot( ) + 
  geom_sf( data = d ) +
  geom_sf( data = sea, fill = NA, color = "red" )+
  theme_minimal( ) 

```

## `st_within` for Subsetting 1023-EZ Data

- This function will play a key role in subsetting the 1023 data for overlaying on an MSA map

- Idea is to subset the 1023 EZ shapefiles to those sharing geometries within an MSA

## Example

```{r,echo = FALSE, fig.align = 'center', fig.height = 3, fig.width = 3,warning = FALSE, message = FALSE }
##  subset 1023 data sf files based on those having geometries within the MSA
lf <- "/Volumes/My Passport for Mac/Urban Institute/Summer Projects/Geospatial Dashboard/Large-Files-Bank"

setwd( lf )
np <- readRDS( "NONPROFITS-2014-2021v6.rds" )

# Convert lat/long to a sf
np.sf <- np %>%
  filter( is.na( lat ) == F ) %>% # st_as_sf does not allow missing values in coordinate columns; n = 167 missing
  st_as_sf( coords = c( "lon", "lat" ), crs = "NAD83" )

# subset
s2 <- st_within( np.sf, sea )

these.2 <- map_lgl( s2, function( x ) {
  if ( length( x ) == 1 ) {
    return( TRUE )
  } else {
    return( FALSE )
  }
} )


np.sf.2 <- np.sf [these.2, ]

ggplot( ) + 
  geom_sf( data = d ) +
  geom_sf( data = np.sf.2 ) + 
  geom_sf( data = sea, fill = NA, color = "red" ) +
  theme_minimal( )

```


## Issues Remain

- Again, even at this level, dots on a map are not very informative

- Let's try chloropleths and other approaches for modeling density

- *Problem*: tract FIPS codes are only available for those addresses geocoded through the Census Geocoder service ( we also used Google Geocoder API and other approaches )

- *Solution*: we can harness data using from the `tigris` package and the `coords_to_fips` function from the `fipio` package



## Nonprofit Density ( Organizations/1,000 inhabitants )

```{r,echo = FALSE, fig.align = 'center', fig.height = 3, fig.width = 3,warning = FALSE, message = FALSE}
# Binning function import
quant.cut <- function( var,x,df ){
xvec <- vector( )
for ( i in 1:x ){
  xvec[i] <- i/x
}

qs <- c( min( df[[var]],na.rm = T ), quantile( df[[var]],xvec,na.rm = T ) )

df[['new']] = x+1 #initialize variable


for ( i in 1:( x ) ){
  df[['new']] <- ifelse( df[[var]]<qs[i+1] & df[[var]]>= qs[i],
                         c( 1:length( qs ) )[i],
                         ifelse( df[[var]] == qs[qs == max( qs )],x,df[['new']] ) )
}

return( df[['new']] )
}


# compound fips code
np.sf.2$fips <- paste0( np.sf.2$STATEFIPS, np.sf.2$COUNTYFIPS, np.sf.2$TRACTFIPS )

dat.geo <- st_join( d, np.sf.2, join = st_nearest_feature, left = T )

# count rows by FIPS, create density measure
c.dens <- dat.geo %>%
  group_by( fips ) %>%
  mutate( pop = ceiling( p.density * ( ALAND*1e-06 ) ), # convert sq. meters to sq. km
          n = n( ), 
          o.density = ( n / pop ) * 1000 ) %>%
  distinct( fips, pop , o.density , geometry )


# bin density for plotting
c.dens$o.den.bin = factor( quant.cut( var = 'o.density', x = 5 ,df = c.dens ) )


## plotting

ggplot( ) + 
  geom_sf( data = d ) +
  geom_sf( data = sea, fill = NA, color = "red" )+
  geom_sf( c.dens,
          mapping = aes( fill = o.den.bin ),
          color = NA, size = 0.5 ) +
scale_fill_manual( 'Quintile', values = c( "#DCE319FF", "#55C667FF", "#238A8DFF",
                             "#39568CFF","#440154FF" ) ) +
  theme_minimal( ) 
```

## Landing Page Map

```{r,echo = FALSE, fig.align = 'center', fig.height = 4, fig.width = 4.4,warning = FALSE, message = FALSE}
quant.cut <- function( var,x,df ){
xvec <- vector( )
for ( i in 1:x ){
  xvec[i] <- i/x
}

qs <- c( min( df[[var]],na.rm = T ), quantile( df[[var]],xvec,na.rm = T ) )

df[['new']] = x+1 #initialize variable


for ( i in 1:( x ) ){
  df[['new']] <- ifelse( df[[var]]<qs[i+1] & df[[var]]>= qs[i],
                         c( 1:length( qs ) )[i],
                         ifelse( df[[var]] == qs[qs == max( qs )],x,df[['new']] ) )
}

return( df[['new']] )
}

# Convert lat/long to a sf
np.sf <- np.b %>%
  filter( is.na( lat ) == F ) %>% # st_as_sf does not allow missing values in coordinate columns; n = 167 missing
  st_as_sf( coords = c( "lon", "lat" ), crs = "NAD83" )


# no. of NPOs by county FIPS
n.ct <- as.data.frame( np.sf %>%
                         group_by( fips_ct ) %>%
                         mutate( n = n( ),
                                 fips_ct = as.numeric( fips_ct ) ) %>% # count of NPO's within county FIPS
                         distinct( fips_ct, n ) )


# county population 

invisible( census_api_key( "apigoeshere", install = TRUE,
               overwrite = TRUE ) )
invisible( readRenviron( "~/.Renviron" ) )

pop.ct <- as.data.frame( get_acs( geography = "county", 
                                 variables = c( pop = "B07401_001" ), 
                                 year = 2020 ) %>%
                           select( fips_ct = GEOID, estimate ) )


## merge
n.pop.merge <- pop.ct %>%
  mutate( fips_ct = as.numeric( fips_ct ) ) %>%
  left_join( n.ct, . ) %>%
  mutate( dens = n / estimate ) %>%
  filter ( is.na( dens ) == F ) %>%
  mutate( dens.q = factor( quant.cut( var = 'dens', x = 5 ,df = . ) ) )


ct <- st_transform( urbnmapr::get_urbn_map( map = "counties", sf = TRUE ), crs = 'NAD83' ) %>%
  mutate( county_fips = as.numeric( county_fips ) ) %>%
            left_join( ., n.pop.merge, by = c( 'county_fips' = 'fips_ct' ) )




ggplot( ) + geom_sf( ct,
                   mapping = aes( fill = dens.q ),
                   color = NA, size = 0.5 )+
  scale_fill_manual( 'Quintile', values = c( "#DCE319FF", "#55C667FF", 
                                            "#238A8DFF", "#39568CFF","#440154FF" ) ) +
  theme_minimal( )+
  theme( legend.position = 'none' )
```


## Next Steps


- Finalize metrics to be used for NPO

- Begin working with board members data and finalizing metrics with this dataset

- Scale up to dashboard and get prototype/skeleton in place

- Conceptualizing widgets that will: i ) allow users to zoom into MSAs/other regions ii ) select a metric


